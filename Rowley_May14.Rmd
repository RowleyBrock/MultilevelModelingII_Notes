---
title: "Rowley_May14"
author: "Brock Rowley"
date: "5/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(carData)
library(brms)
library(tidybayes)

popular <- read_csv(here::here("data", "popularity.csv"))
mplus_d <- read_csv(here::here("data", "mplus920.csv"))
```

```{r models}
model1 <- lmer(score ~ 1 + (1|distid), data = mplus_d)

model2 <- lmer(score ~ baseline + (1|schid) + (1|distid),
     data = mplus_d)

model3 <- lmer(score ~ baseline * dist_ses +
       (baseline|schid) + (1|distid),
     data = mplus_d)

model4 <- lmer(score ~ baseline + sch_treatment + dist_ses + 
       (baseline|schid) + (1|distid),
     data = mplus_d)

model5 <- lmer(score ~ baseline * sch_treatment + dist_ses + 
       (baseline|schid) + (sch_treatment|distid), # missed the random effect here
     data = mplus_d)

model6 <- lmer(score ~ baseline * sch_treatment + dist_ses + 
       (baseline|schid) + 
       (baseline * sch_treatment||distid), # double pipe for just variance not covariance
           data = mplus_d)

final_model <- lmer(score ~ baseline * sch_treatment * dist_ses + 
             (baseline|schid) + (baseline|distid),
           data = mplus_d)
```

```{r carData}
iqs <- carData::Burt$IQbio
iqs

ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15))
dnorm(80, mean = 100, sd = 15)
```

# Likelihood
## What's the likelihood of a score of 80, assuming this distribution?

```{r echo = FALSE, fig.height = 6}
ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15)) +
  geom_point(aes(y = y),
             data.frame(x = 80, y = dnorm(80, 100, 15)),
             color = "magenta") +
  geom_segment(x = 80, xend = 80, y = 0, yend = dnorm(80, 100, 15),
               linetype = "dashed",
               color = "magenta") +
  geom_segment(x = -Inf, xend = 80, 
               y = dnorm(80, 100, 15), yend = dnorm(80, 100, 15),
               linetype = "dashed",
               color = "magenta")

dnorm(80, mean = 100, sd = 15)
```
---
# Likelihood of the data
We sum the likelihood to get the overall likelihood of the data. However, this leads to very small numbers. Computationally, it's easier to sum the *log* of these likelihoods.

```{r }
dnorm(iqs, mean = 100, sd = 15, log = TRUE)
```

```{r }
sum(dnorm(iqs, mean = 100, sd = 15, log = TRUE))
```

# Alternative distributions
## What if we assumed the data were generated from an alternative distribution, say $IQ_i \sim N(115, 5)$?

```{r }
sum(dnorm(iqs, mean = 115, sd = 5, log = TRUE))
```

## The value is *much* lower. In most models, we are estimating $\mu$ and $\sigma$, and trying to find values that *maximize* the sum of the log likelihoods.

# Visually
## The real data generating distribution

```{r echo = FALSE}
iq_likelihood <- data.frame(x = iqs, y = dnorm(iqs, 100, 15))

ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 100, 15)) +
  geom_point(aes(y = y),
             iq_likelihood,
             color = "magenta") +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y),
               iq_likelihood,
               linetype = "dashed",
               color = "magenta")
```

# Visually
## The poorly fitting one

```{r echo = FALSE}
iq_likelihood2 <- data.frame(x = iqs, y = dnorm(iqs, 120, 5))

ggplot(data.frame(x = c(60, 140)), aes(x)) +
  stat_function(fun = ~dnorm(., 120, 5)) +
  geom_point(aes(y = y),
             iq_likelihood2,
             color = "magenta") +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y),
               iq_likelihood2,
               linetype = "dashed",
               color = "magenta")
```

# Non-Bayesian
## In a frequentist regression model, we would find parameters that *maximize* the likelihood. Note - the distributional mean is often conditional.

## This is part of why I've come to prefer notation that emphasizes the data generating process.

# Example
## I know we've talked about this before, but a simple linear regression model like this

```{r }
m <- lm(IQbio ~ class, data = carData::Burt)
```

## generally displayed like this

```{r echo = FALSE}
equatiomatic::extract_eq(m, font_size = "normalsize")
```

## But we could display the same thing like this
$$
\begin{align}
\operatorname{IQbio} &\sim N(\widehat{\mu}, \widehat{\sigma}) \\
\widehat{\mu} = \alpha &+ \beta_{1}(\operatorname{class}_{\operatorname{low}}) + \beta_{2}(\operatorname{class}_{\operatorname{medium}})
\end{align}
$$
# Priors
## Bayesian posterior

$$
\text{posterior} = \frac{ \text{likelihood} \times \text{prior}}{\text{average likelihood}}
$$
## The above is how we estimate with Bayes.
## In words, it states that our updated beliefs (posterior) depend on the evidence from our data (likelihood) and our prior knowledge/conceptions/information (prior).
## Our prior will shift in accordance with the evidence from the data

# Basic example
## Let's walk through a basic example where we're just estimating a mean. We'll assume we somehow magically know the variance. Please follow along.

## First, generate some data
```{r }
set.seed(123)
true_data <- rnorm(50, 5, 1) # give me some sample from a distribution. 50 data, mean of 5, SD 1
```

# Grid search
## We're now going to specify a grid of possible means for our data. Let's search anywhere from -3 to 12 in 0.1 intervals.

```{r }
grid <- tibble(possible_mean = seq(-3, 12, 0.1)) # from -3 to 12 by 0.1
```

## Next, we'll specify a *prior distribution*. That is - how likely do we *think* each of these possible means are?

## Let's say our best guess is $mu = 2$. Values on either side of $2$ should be less likely.

--
```{r }
prior <- dnorm(grid$possible_mean, mean = 2, sd = 1) # how likely are these means?
```

# Plot our prior

```{r }
grid %>%
  mutate(prior = prior) %>%
  ggplot(aes(possible_mean, prior)) +
  geom_line()
```

## Note that the *strength* of our prior depends on the standard deviation

## This would be our best guess as to where the data would fall *before* observing the data.

# Look at other priors

```{r fig.height = 6}
grid %>%
  mutate(prior1 = dnorm(possible_mean, mean = 2, sd = 1),
         prior2 = dnorm(possible_mean, mean = 2, sd = 2),
         prior3 = dnorm(possible_mean, mean = 2, sd = 3)) %>%
  ggplot(aes(possible_mean)) +
  geom_line(aes(y = prior1)) + 
  geom_line(aes(y = prior2), color = "cornflowerblue") +
  geom_line(aes(y = prior3), color = "firebrick")
```

# Set prior
* Let's go with a fairly conservative prior, with $\mu = 2, \sigma = 3$.
* We also need to normalize it so the probability sums to 1.0

```{r }
grid <- grid %>%
  mutate(prior = dnorm(possible_mean, mean = 2, sd = 3),
         prior = prior / sum(prior)) # normalize
```

# Observe 1 data point
```{r }
grid <- grid %>%
  mutate(likelihood = dnorm(true_data[1], possible_mean, 2))
grid
```

# Compute posterior
```{r }
grid <- grid %>%
  mutate(posterior = likelihood * prior,
         posterior = posterior / sum(posterior)) # normalize
```

# Plot
```{r }
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

# Observe a second data point
## The old posterior becomes our new prior
```{r }
grid <- grid %>%
  mutate(likelihood = dnorm(true_data[2], possible_mean, 2),
         posterior = likelihood * posterior,
         posterior = posterior / sum(posterior))
```

# Plot
```{r }
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

# Observe a third data point
```{r }
grid <- grid %>%
  mutate(likelihood = dnorm(true_data[3], possible_mean, 2),
         posterior = likelihood * posterior,
         posterior = posterior / sum(posterior))
```

# Plot
```{r }
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

# All the data
```{r }
grid <- grid %>%
  mutate(prior = dnorm(grid$possible_mean, mean = 2, sd = 3),
         prior = prior / sum(prior),
         posterior = prior) # best guess before seeing data

for(i in seq_along(true_data)) {
  grid <- grid %>%
    mutate(likelihood = dnorm(true_data[i], possible_mean, 2),
           posterior = likelihood * posterior,
           posterior = posterior / sum(posterior))
}
```

```{r }
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

# Posterior
* We can summarize our posterior distribution
* This is a fundamental difference between Bayesian & frequentist approaches
  + In Bayes, our data is assumed fixed, our parameters random
  + In frequentist, our data is assumed random, our parameters fixed

## Most likely?
```{r }
grid %>%
  filter(posterior == max(posterior))
```

# Sampling
* Now that we have a posterior distribution, we can sample from it to help us with inference.
* Each possible mean should be sampled in accordance with its probability specified by the posterior.

## Let's draw 10,000 samples
```{r }
posterior_samples <- sample(grid$possible_mean, 
                            size = 10000,
                            replace = TRUE,
                            prob = grid$posterior)
```

# Inference
## First, let's plot the samples
```{r }
ggplot(data.frame(sample = posterior_samples), aes(sample)) +
  geom_histogram(bins = 100)
```

# Central tendency
```{r }
mean(posterior_samples)
median(posterior_samples)
```

## Spread
```{r }
sd(posterior_samples)
```

# Credible intervals
## Let's compute an 80% credible interval
```{r }
tibble(posterior_samples) %>%
  summarize(ci_80 = quantile(posterior_samples, c(0.1, 0.9)))
```

## What's the chance the "true" mean is less than 4.8?
```{r }
sum(posterior_samples < 4.8) / length(posterior_samples) * 100
```

# Ranges
## What's the probability the "true" mean is between 5.2 and 5.5?
```{r }
sum(posterior_samples >= 5.2 & posterior_samples <= 5.5) /
  length(posterior_samples) * 100
```

## Greater than 4.5?
```{r }
sum(posterior_samples > 4.5) / length(posterior_samples) * 100
```
## Note this is much more natural than frequentist statistics

# Change our prior
## Let's try again with a tighter prior
```{r }
grid <- grid %>%
  mutate(prior = dnorm(grid$possible_mean, mean = 2, sd = 0.1),
         prior = prior / sum(prior),
         posterior = prior) # best guess before seeing data

for(i in seq_along(true_data)) {
  grid <- grid %>%
    mutate(likelihood = dnorm(true_data[i], possible_mean, 2),
           posterior = likelihood * posterior,
           posterior = posterior / sum(posterior))
}
```

```{r fig.height = 5}
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

```{r }
grid %>%
  filter(posterior == max(posterior))
```

# More data
## Same thing, but this time with tons of data
```{r }
true_data <- rnorm(5000, 5, 1)
grid <- grid %>%
  mutate(prior = dnorm(grid$possible_mean, mean = 2, sd = 0.1),
         prior = prior / sum(prior),
         posterior = prior) # best guess before seeing data

for(i in seq_along(true_data)) {
  grid <- grid %>%
    mutate(likelihood = dnorm(true_data[i], possible_mean, 2),
           posterior = likelihood * posterior,
           posterior = posterior / sum(posterior))
}
```

```{r }
grid %>%
  pivot_longer(-possible_mean) %>%
ggplot(aes(possible_mean, value)) +
  geom_line(aes(color = name))
```

```{r }
grid %>%
  filter(posterior == max(posterior))
```

# Taking a step back
* The purpose of the prior is to include *what you already know* into your analysis
* The strength of your prior should depend on your prior research
* Larger samples will overwhelm priors quicker, particularly if they are diffuse
* Think through the lens of updating your prior beliefs
* This whole framework is quite different, but also gives us a lot of advantages in terms of probability interpretation, as we'll see