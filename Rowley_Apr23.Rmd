---
title: "Rowley_Apr23"
author: "Brock Rowley"
date: "4/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(equatiomatic)
library(TMB)
library(dfoptim)
library(sundry)
library(nlme)
library(performance)

popular <- read_csv(here::here("data", "popularity.csv"))

willett <- read_csv(here::here("data", "willett-1988.csv"))
```

```{r data_model}
m <- lmer(popular ~ extrav + (extrav|class), popular,
          control = lmerControl(optimizer = "bobyqa"))
# extrav is a measure of extraversion.

arm::display(m)

slr <- lm(popular ~ extrav, popular)

ggplot(popular, aes(extrav, popular)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm")

# ˆpopular = 3.27 + 0.35(extrav)
```


```{r sample_preds}
sample_preds <- popular %>%
  group_by(class) %>%
  slice(1) %>%
  ungroup %>%
  slice(1:4)
sample_preds
```

```{r grab_params}
# Fixed effects
f <- fixef(m)
f

# Classroom deviations
r <- ranef(m)
r

# Predictions depend on classroom
# Works just like simple linear regression
sample_preds[1, ]

# ˆpopular = 2.46 + 0.49 × 5 = 4.91

f[1] + f[2]*5
```

```{r random_effects}
# Add in the random effects for the corresponding classroom
head(r$class)
# ˆpopular = (2.46 + 0.34) + (0.49 + −0.03) × 5 = 5.10

class1 <- r$class[1, ]
class1

(f[1] + class1[1]) + (f[2] + class1[2])*5
```

```{r predictions}
sample_preds

head(r$class, n = 4)

fixef(m)

predict(m, newdata = sample_preds)
```

```{r plotting}
# We can use the expand.grid() function to create different conditions. Compare slopes across the first five classrooms
conditions <- expand.grid(extrav = 1:10, class = 1:5)
head(conditions)
tail(conditions)

# Make predictions
conditions %>%
  mutate(model_pred = predict(m, newdata = conditions))

# Plot
conditions %>%
  mutate(model_pred = predict(m, newdata = conditions)) %>%
  ggplot(aes(extrav, model_pred)) +
  geom_line(aes(group = class))

# One more quick example
# Model an interaction
m2 <- lmer(popular ~ extrav*sex + (extrav|class), popular,
          control = lmerControl(optimizer = "bobyqa"))

arm::display(m2)

# Marginal effect
# Interaction between extraversion and sex
conditions2 <- expand.grid(extrav = 1:10,
                           sex = c("girl", "boy"),
                           class = 0) %>%
  mutate(pred = predict(m2,
                        newdata = .,
                        allow.new.levels = TRUE))
conditions2

#Plot
ggplot(conditions2, aes(extrav, pred)) +
  geom_line(aes(color = sex))
```

```{r notation}
m <- lm(mpg ~ disp + hp + drat, data = mtcars)

# simulate
n <- 1000
intercept <- 100
b1 <- 5
b2 <- -3
b3 <- 0.5
sigma <- 4.5

set.seed(123)
x1 <- rnorm(n, sd = 1)
x2 <- rnorm(n, sd = 2)
x3 <- rnorm(n, sd = 4)

# Create y-hat
yhat <- intercept + b1*x1 + b2*x2 + b3*x3

# Generate data & test
sim <- rnorm(n, yhat, sigma)
summary(lm(sim ~ x1 + x2 + x3))
```

```{r simple_example}
hsb_m0 <- lmer(math ~ ses + (1|sch.id), data = hsb)

# Set some parameters
j <- 30 # 30 schools
nj <- 50 # 50 students per school

# Simulate the school distribution
a_j <- rnorm(j, 0, 2.18)

# For each school, simulate nj obs from leve 1 model, adding in the school deviation

# Using a for() loop here in an effort to be transparent
school_scores <- vector("list", j)
ses <- vector("list", j)
for(i in 1:j) {
  ses[[i]] <- rnorm(nj)
  school_scores[[i]] <- rnorm(nj, 
                              12.66 + 2.39*ses[[i]] + a_j[i], 
                              6.09)
}
sim_df <- data.frame(
  scid = rep(1:j, each = nj),
  ses = unlist(ses),
  score = unlist(school_scores)
)

# Test it out
sim_m0 <- lmer(score ~ ses + (1|scid), data = sim_df)
summary(sim_m0)

# Add a school-level predictor
hsb_m1 <- lmer(math ~ ses + sector + (1|sch.id), data = hsb)
extract_eq(hsb_m1)

# Add a random slope
hsb_m2 <- lmer(math ~ ses + sector + (ses|sch.id), data = hsb)
extract_eq(hsb_m2)

# Include sector as a predictor of the relation between ses and math
hsb_m3 <- lmer(math ~ ses * sector + (ses|sch.id), data = hsb,
               control = lmerControl(optimizer = "nmkbw"))
extract_eq(hsb_m3)

#Even more complicagted
hsb_m4 <- lmer(
  math ~ ses * sector + minority + female + meanses + size +
    (ses + minority + female|sch.id), 
  data = hsb
)
extract_eq(hsb_m4)
```

```{r multiple_levels}
head(sim_longitudinal)

# Four levels, model doesn't really fit again
sl_m <- lmer(
  score ~ wave*treatment + group + prop_low +
    (wave|sid) + (wave + treatment| school) + (1|district),
  data = sim_longitudinal
)
extract_eq(sl_m)
```

```{r standard_OLS}
bad <- lm(opp ~ time, data = willett)
summary(bad)

# Fit a parallel slopes model with the Willett data
w0 <- lmer(opp ~ time + (1|id), willett)

# Use sundry to pull the residual variance-covariance
w0_rvcv <- pull_residual_vcov(w0)

# Sparse matrix - view with image()
image(w0_rvcv)

# Pull first few rows/cols
w0_rvcv[1:8, 1:8]

# Look at the model output
arm::display(w0)

# Extract the variance components from the model
vars_w0 <- as.data.frame(VarCorr(w0))
vars_w0
# The diagonals are given by sum(vars_w0)$vcov while the off-diagonals are just the intercept variance
```

```{r composite_residual}
w1 <- lmer(opp ~ time + (time|id), willett)
w1_rvcv <- pull_residual_vcov(w1)
w1_rvcv[1:4, 1:4]

# Unstructured
# The model we fit has an unstructured variance co-variance matrix. While each block is the same, every element of the block is now estimated.

# Get the pieces
vars_w1 <- as.data.frame(VarCorr(w1))
int_var <- vars_w1$vcov[1]
slope_var <- vars_w1$vcov[2]
covar <- vars_w1$vcov[3]
residual <- vars_w1$vcov[4]

# Calculate
diag(w1_rvcv[1:4, 1:4])
residual + int_var
residual + int_var + 2*covar + slope_var
residual + int_var + (2*covar)*2 + slope_var*2^2
residual + int_var + (2*covar)*3 + slope_var*3^2

# Calculate a few
w1_rvcv[1:4, 1:4]
int_var + covar*(1 + 0) + slope_var*1*0
int_var + covar*(2 + 1) + slope_var*2*1
int_var + covar*(3 + 2) + slope_var*3*2
int_var + covar*(2 + 0) + slope_var*2*0
```

```{r fit}
ar <- gls(opp ~ time, 
          data = willett,
          correlation = corAR1(form = ~ 1|id))
summary(ar)

# Extract composite residual
# all of them
cm_ar <- corMatrix(ar$modelStruct$corStruct)
# just the first (they're all the same)
cr_ar <- cm_ar[[1]]
cr_ar

# Multiply the correlation matrix by the model residual variance to get the covariance matrix
cr_ar * sigma(ar)^2

# Confirming calculations
sigma(ar)^2
sigma(ar)^2*0.8249118
sigma(ar)^2*0.8249118^2
sigma(ar)^2*0.8249118^3
```

```{r varIdent}
har <- gls(
  opp ~ time,
  data = willett,
  correlation = corAR1(form = ~ 1|id),
  weights = varIdent(form = ~1|time)
)
summary(har)
```

```{r extract_compute_comp_residual}
cm_har <- corMatrix(har$modelStruct$corStruct)[[1]]
var_struct <- har$modelStruct$varStruct
vars <- coef(var_struct, unconstrained = FALSE, allCoef = TRUE)
vars <- matrix(vars, ncol = 1)
cm_har * sigma(har)^2 *
  (vars %*% t(vars)) # multiply by a mat of vars
```

```{r toeplitz}
# Fit
toep <- gls(opp ~ time, 
            data = willett,
            correlation = corARMA(form = ~ 1|id, p = 3))
summary(toep)

# Extract/compute composite residual
# Same as with autoregressive - just multiply the correlation matrix by the residual variance.
cr_toep <- corMatrix(toep$modelStruct$corStruct)[[1]] 
cr_toep * sigma(toep)^2
```

```{r comparing_fits}
# We have slight evidence here that the Toeplitz structure fits
compare_performance(ar, har, toep, w1,
                    metrics = c("AIC", "BIC"),
                    rank = TRUE) %>%
  as_tibble()
```

